---
title: "STAT 3032 - HW1"
author: "Saksham Goel"
date: "February 3, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1 - Box Office Ticket Sales

### Introducing the Dataset

This section is dedicated to see and understand the data that was provided from the weekly reports about the box office ticket sales for plays in Broadway in New York. The data being observed is of the week of October 11- 17, 2004. The dataset contains data about the gross box office results for the current week October 11-17, 2004 and that of the previous week October 3-10, 2004.

The data table is as follows:
```{r q1_view_data}
ticketSales = read.csv("playbill.csv", header=T)
ticketSales[,]
```

### Visualizing the Dataset
The dataset can be visalized through a scatterplot in the figure below:
```{r q1_plot_data}
plot(x = ticketSales$LastWeek, y = ticketSales$CurrentWeek)
```

### Fitting a linear model
The linear trend in the scatterplot seems strong, which means that a linear regression model is appropriate. The linear model that we are trying to fit is of the form:

\begin{center}$\hat{Y} = \hat{\beta_0} + \hat{\beta_1}X$\end{center}

here,
X = Gross Box Office results for Previous Week


Y = Gross Box Office results for Current Week. 

Through this linear model we are trying to predict the value of actual gross box office results of current week (Y) using the gross box office results of the previous week (X). The following snippet of the R-Code fits a linear model on the data, prints out the values of the intercept and slope and also provides a summary of the fitted model.
```{r q1_fit_model}
mod = lm( ticketSales$CurrentWeek ~ ticketSales$LastWeek )
mod
```

After fitting the model and observing the values of the parameters $\hat{\beta_0}, \hat{\beta_1}$ we find that:

\begin{center} $ \hat{\beta_0} = 6804.8860 $ \end{center}
\begin{center} $\hat{\beta_1} = 0.9821$ \end{center}


The summary of the model is as follows:
```{r q1_fit_model2}
summary(mod)
```
The summary provides us with a lot of useful values that we will use in the upcoming sections to calculate the Confidence Intervals, Prediction Intervals and perform Hypothesis Testing. Some of the values are as follows:

Sum of Residuals Square (s) = 18010


Degrees of freedom (n-2) = 16


Combining all the information from the previous sections the final fitted model looks as follows:

\begin{center}$Y = 6804.8860 + (0.9821 * X)$\end{center}

### Finding a 95% Confidence Interval for $\beta_1$

A $100(1-\alpha)\%$ confidence interval for $\hat{\beta_1}$ is given by the following formula:

\begin{center}
\begin{math}
\hat{\beta_{1}} -  t_{ \frac{\alpha}{2},n-2}\frac{s}{ \sqrt{ S_{XX} } } \leq \beta_1 \leq \hat{\beta_{1}} + t_{ \frac{\alpha}{2},n-2}\frac{s}{ \sqrt{ S_{XX} } }
\end{math}
\end{center}

The following snippet of code is used to find the 95% confidence interval for $\hat{\beta_1}$:

```{r q1_CI}
bet1_hat = 0.9821 #found using the linear model
x_col = ticketSales$LastWeek
y_col = ticketSales$CurrentWeek
x_bar = mean(x_col)
#x_bar = 622186.6
y_bar = mean(y_col)
#y_bar = 617842.8
sxx = sum((x_col-x_bar)^2)
#sxx = 1.557916 * 10^12
sxy = sum((x_col-x_bar)*(y_col-y_bar))
#sxy = 1.53 * 10^12
#sxy/sxx = 0.9821
s = 18010 #found through the summary of the linear model
t_mult = 2.120 # found using the T Table, equal to t0.25 for 16 degrees of freedom
beta1CIlower = bet1_hat - (t_mult*(s/sqrt(sxx)))
beta1CIupper = bet1_hat + (t_mult*(s/sqrt(sxx)))
```

The values we found are as follows:

\begin{math} \bar{x} = 622186.6 \end{math}\
\begin{math} \bar{y} = 617842.8 \end{math}\
\begin{math} S_{XX} = 1.557916 * (10 ^12) \end{math}\
\begin{math} S_{XY} = 1.53 * (10 ^12) \end{math}\
\begin{math} \hat{\beta_{1}} = 0.9820815 \end{math}\



The $95\%$ confidence interval can thus be given as follows: 
\begin{center}
\begin{math}
0.9515101 \leq \beta_{1} \leq 1.01269
\end{math}
\end{center}

Yes we can say that $1$ is a reasonable value for $\beta_1$ because $1$ lies in the $95\%$ Confidence Interval of $\beta_1$

### Hypothesis Testing for $\beta_0$
We need to run a Hypothesis Test for $\beta_0$ given by:
\begin{center}
\begin{math}
Null \big(H_{0}\big) \beta_{0}= 10000
\end{math}
\end{center}

\begin{center}
\begin{math}
Alternate \big(H_{1}\big) \beta_{0} \ne 10000
\end{math}
\end{center}

The hypothesis test for $\beta_0$ is given by the following formula:

\begin{center}
\begin{math}
\hat{T} = \frac{\hat{\beta_0} - \beta_0}{s\sqrt{\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}}}}
\end{math}
\end{center}

To find the p-value we then use the t-statistic we got from the previous formula and find the probability as follows:
\begin{center}
\begin{math}
p-val = 2 \cdot P\big( t \geq \mid\hat{T}\mid\big)
\end{math}
\end{center}

The following snippet of code is used to perform the Hypothesis Test for $\beta_0$:

```{r q1_HT}
bet0_hat = 6804.8860 #this value was found using the linear model through r in the previous section
x_col = ticketSales$LastWeek
x_bar = mean(x_col)
#x_bar = 622186.6
sxx = sum((x_col-x_bar)^2)
#sxx = 1.557916 * 10^12
s = 18010 #found through the summary of the linear model
t_mult = 2.120 # found using the T Table, equal to t0.25 for 16 degrees of freedom
n = length(x_col)
beta0test = ((bet0_hat - 10000)/(s * sqrt((1/n) + ((x_bar^2)/sxx))))
#beta0test =  -0.3217422
p_val = 2*pt(-abs(beta0test), df = n-2)
#p_val = 0.7518132
```

After performing the Hypothesis test for $\beta_0$ we find that the $p-value = 0.7518132$ which is greater than $0.05$, hence we cannot reject the null hypothesis. This result means that it is very likely (probability of 75.18%) that we will see a value of $\beta_0$ as much as here. Because the probability is high we really cannot reject the Null Hypothesis $H_0$ and our hypothesis testing suggests that the value of $\beta_0$ can be 10,000.

### Point Estimate for new Y

Through the previous sections we know that:
\begin{center}$Y = 6804.8860 + (0.9821 * X)$\end{center}

So to find the point estimate of Y using the fitted model we get:
```{r point_Estimate}
X = 400000 #X value for which we need to find the estimated value
Y = 6804.8860 + (0.9821 * X)
#Y = 399644.9
```

Through this above snippet of code we get the point estimate of $Y$ at $X = 400,000$ as $Y = 399644.9$.


### Prediction Interval for new Y
There is no use of a Linear Model if we cannot predict new values of the parameter Y through X. In this section we will construct a prediction interval of the Y value using the formulas provided in the book. The prediction interval of Y is given by the formula as follows:

\begin{center}
\begin{math}
\hat{y}_{curr} \pm t_{\alpha/2,n-2} s \sqrt{1+\dfrac{1}{n}+\dfrac{(x_{curr}-\bar{x})^2}{\sum(x_i-\bar{x})^2}}
\end{math}
\end{center}

The following snippet of code helps us to find the upper and lower limits of the Prediction Interval for the new Y as follows:

```{r q1_PI}
#fitted model = 
# Y = 6804.8860 + 0.9821 * X
x_curr = 400000 #the value at which we need to find the prediction interval
y_hat = 6804.8860 + (0.9821 * x_curr)
#y_hat = 399644.9.
x_col = ticketSales$LastWeek
x_bar = mean(x_col)
#x_bar = 622186.6
sxx = sum((x_col-x_bar)^2)
#sxx = 1.557916 * 10^12
s = 18010 #found through the summary of the linear model
t_mult = 2.120 # found using the T Table, equal to t0.25 for 16 degrees of freedom
n = length(x_col)
yPILower = y_hat - (t_mult * s * sqrt(1 + (1/n) + (((x_curr - x_bar)^2)/sxx) ))
yPIUpper = y_hat + (t_mult * s * sqrt(1 + (1/n) + (((x_curr - x_bar)^2)/sxx) ))
#yPILower = 359833
#yPIUpper = 439456.8
```

Through the above snippet of code the resulting $95\%$ prediction interval is given as follows:

\begin{center}
\begin{math}
359833 \leq Y \leq 439456.8
\end{math}
\end{center}

Because the prediction interval does not contain the value $450,000$ in it, we cannot use that value as a prediction. We are basing all of our decision based on the abover prediction interval. If the data would have been different there could have been enough evidence to say that the value lies in the prediction interval, however because for the current dataset it does not exist in the prediction interval, we can say with 95% confidence that this value would not be a good prediction.

### Comment:
I think that is a statement that could be used frequently to describe the sales record for the broadway show, because the linear model also seems to follow a trend as seen. Other than that all the results from the previous computations seems to provide a good enough evidence to support this claim. I am arguing in favor for this statement because of the linear model that I was able to fit which contains a slope term of 0.9821 which when formed a 95% confidence interval for the average value, contains 1 in it which shows that the data on average is expected to have a equality coorelation to a very high degree.

# Question 2 - Processing Time of Invoices

### Introducing the Dataset

This section is dedicated to see and understand the data that was provided from a large company about the number of invoices processed and the total processing time for those invoices. The data being observed is of a whole month. 
The data table is as follows:
```{r q2_view_data}
invoiceTime = read.table("invoices.txt", header = T)
invoiceTime[,]
```


### Visualizing the Dataset
The dataset can be visalized through a scatterplot in the figure below:
```{r q2_plot_data}
plot(x = invoiceTime$Invoices, y = invoiceTime$Time)
```

### Fitting a linear model
The linear trend in the scatterplot seems strong, which means that a linear regression model is appropriate. The linear model that we are trying to fit is of the form:

\begin{center}$\hat{Y} = \hat{\beta_0} + \hat{\beta_1}X$\end{center}

here,
X = Number of Invoices


Y = Total Time required to process those Invoices

Through this linear model we are trying to predict the value of total time required to process invoices (Y) using the number of invoices (X). Using the summary table from page 40 in the book we find that:

\begin{center} $ \hat{\beta_0} = 0.64171 $ \end{center}

\begin{center} $\hat{\beta_1} = 0.01129 $ \end{center}

The summary provides us with a lot of other useful values as well that we will use in the upcoming sections to calculate the Confidence Intervals, Prediction Intervals and perform Hypothesis Testing. Some of the values are as follows:

Sum of Residuals Square (s) = 0.3298


Degrees of freedom (n-2) = 28


Combining all the information from the previous sections the final fitted model looks as follows:

\begin{center}$Y = 0.6417099 + (0.0112916 * X)$\end{center}


### Finding a 95% Confidence Inrerval $\beta_0$
A $100(1-\alpha)\%$ confidence interval for $\hat{\beta_0}$ is given by the following formula:

\begin{center}
\begin{math}
\hat{\beta_{0}} -  t_{ \frac{\alpha}{2},n-2}\cdot s \cdot \sqrt{\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}}} \leq \beta_0 \leq \hat{\beta_{0}} +  t_{ \frac{\alpha}{2},n-2}\cdot s \cdot \sqrt{\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}}}
\end{math}
\end{center}

First finding the confidence interval by just using the summary table given on Page 40. We see:


$\hat{\beta_0}$ = 0.6417099


StdError($\beta_0$) = 0.1222707


$n$ = 30


$s$ = 0.3298


finding the value of the t multiplier $t_{\alpha/2,n-2}$ using qt() we get t_mult = 2.048.
So the below R code helps in computing the values:
```{r q2_CI_1}
bet0_hat = 0.6417099 #using the linear model fit from prevous section
stderror = 0.1222707
n = 30
t_mult = 2.048 #found using the T Table, equal to t0.25 for 28 degrees of freedom
beta0CIlower = bet0_hat - ( t_mult * stderror )
beta0CIupper = bet0_hat + ( t_mult * stderror )
#beta0CIlower = 0.3912
#beta0CIupper = 0.8921
```

The 95% confidence interval found using the above values is as follows:
\begin{center}
\begin{math}
0.3912 \leq \beta_{0} \leq 0.8921
\end{math}
\end{center}

The following snippet of code is used to find the $95\%$ confidence interval for $\hat{\beta_0}$ by using the R code from scratch and not using the value from the Summary:

```{r q2_CI}
bet0_hat = 0.6417099 #using the linear model fit from prevous section
x_col = invoiceTime$Invoices
x_bar = mean(x_col)
#x_bar = 130.0333
sxx = sum((x_col-x_bar)^2)
#sxx = 162367
n = length(x_col)
#n = 30
s = 0.3298 #found through the summary of the linear model
t_mult = 2.048 #found using the T Table, equal to t0.25 for 28 degrees of freedom
beta0CIlower = bet0_hat - ( t_mult * s * sqrt( (1/n) + ((x_bar^2)/(sxx)) ) )
beta0CIupper = bet0_hat + ( t_mult * s * sqrt( (1/n) + ((x_bar^2)/(sxx)) ) )
#beta0CIlower = 0.3912792
#beta0CIupper = 0.8921406
```

The $95\%$ confidence interval can thus be given as follows: 
\begin{center}
\begin{math}
0.3912792 \leq \beta_{0} \leq 0.8921406
\end{math}
\end{center}


We can easily observe that the two methods give the same value for the confidence interval.


### Hypothesis Testing for $\beta_1$
We need to run a Hypothesis Test for $\beta_0$ given by:
\begin{center}
\begin{math}
Null \big(H_{0}\big) \beta_{1}= 0.01
\end{math}
\end{center}

\begin{center}
\begin{math}
Alternate \big(H_{1}\big) \beta_{1} \ne 0.01
\end{math}
\end{center}

The hypothesis test for $\beta_0$ is given by the following formula:

\begin{center}
\begin{math}
\hat{T} = \frac{\hat{\beta_1} - \beta_1}{\frac{s}{\sqrt{S_{XX}}}}
\end{math}
\end{center}

To find the p-value we then use the t-statistic we got from the previous formula and find the probability as follows:
\begin{center}
\begin{math}
p-val = 2 \cdot P\big( t \geq \mid\hat{T}\mid\big)
\end{math}
\end{center}

First we will do the Hypothesis testing using the values from the summary table on Page 40. We see:


$\hat{\beta_1}$ = 0.0112916


StdError($\beta_1$) = 0.0008184


$n$ = 30


$s$ = 0.3298


So the below R code helps in computing the values:
```{r q2_HT_1}
bet1_hat = 0.0112916
stderror = 0.0008184
n = 30
beta1test = ( ( bet1_hat - 0.01 )/( stderror ) )
#beta1test = 1.57807
p_val = 2*pt(-abs(beta1test), df = n-2)
#p_val = 0.1257
```

After performing the Hypothesis test for $\beta_1$ we find that the $p-value = 0.1257$. Now we will compare the p-value for this section with p-value we found using just R code in the code sample below.

The following snippet of code is used to perform the Hypothesis Test for $\beta_1$:

```{r q2_HT}
bet1_hat = 0.0112916 #found using the linear model in the previous sections
x_col = invoiceTime$Invoices
y_col = invoiceTime$Time
x_bar = mean(x_col)
#x_bar = 130.0333
y_bar = mean(y_col)
#y_bar = 2.11
sxx = sum((x_col-x_bar)^2)
#sxx = 162367
sxy = sum((x_col-x_bar)*(y_col-y_bar))
#sxy = 1833.39
#sxy/sxx = 0.0112916
n = length(x_col)
s = 0.3298 #found through the summary of the linear model
t_mult = 2.048 #found using the T Table, equal to t0.25 for 28 degrees of freedom
beta1test = ( ( bet1_hat - 0.01 )/( s/sqrt( sxx ) ) )
#beta1test = 1.57807
p_val = 2*pt(-abs(beta1test), df = n-2)
#p_val = 0.1257819
```

After performing the Hypothesis test for $\beta_1$ we find that the $p-value = 0.1257819$. We see that both ways give us the same p-value which is greater than $0.05$, hence we cannot reject the null hypothesis.

### Point Estimate for new Y

Through the previous sections we know that:
\begin{center}$Y = 0.6417099 + (0.0112916 * X)$\end{center}

So to find the point estimate of Y using the fitted model we get:
```{r point_Estimate_2}
X = 130 #X value for which we need to find the estimated value
Y = 0.6417099 + 0.0112916 * X
#Y = 2.109618
```

Through this above snippet of code we get the point estimate of $Y$ at $X = 130$ as $Y = 2.109618$ which is roughly equal to $127 mins$.


### Prediction Interval for new Y
There is no use of a Linear Model if we cannot predict new values of the parameter Y through X. In this section we will construct a prediction interval of the Y value using the formulas provided in the book. The prediction interval of Y is given by the formula as follows:

\begin{center}
\begin{math}
\hat{y}_{curr} \pm t_{\alpha/2,n-2} s \sqrt{1+\dfrac{1}{n}+\dfrac{(x_{curr}-\bar{x})^2}{\sum(x_i-\bar{x})^2}}
\end{math}
\end{center}

We need to use the value of the summary table to compute the prediction interval. Because $x_{mean}$ is equal to the value of $x_{curr}$ the last term in the standard error for predicting y becomes 0 hence the prediction interval just remains :

\begin{center}
\begin{math}
\hat{y}_{curr} \pm t_{\alpha/2,n-2} s \sqrt{1+\dfrac{1}{n}}
\end{math}
\end{center}

The following snippet of code helps us to find the upper and lower limits of the Prediction Interval for the new Y as follows by just using the values from the summary:

```{r q2_PI}
#fitted model = 
# Y = 0.6417099 + 0.0112916 * X
x_curr = 130 #the value at which we need to find the prediction interval
y_hat = 0.6417099 + (0.0112916 * x_curr)
#y_hat = 2.109618
#n = length(x_col)
n = 30
s = 0.3298 #found through the summary of the linear model
t_mult = 2.048 #found using the T Table, equal to t0.25 for 28 degrees of freedom
yPILower = y_hat - (t_mult * s * sqrt(1 + (1/n) ))
yPIUpper = y_hat + (t_mult * s * sqrt(1 + (1/n) ))
#yPILower = 1.423023
#yPIUpper = 2.796213
```

Through the above snippet of code the resulting $95\%$ prediction interval is given as follows:

\begin{center}
\begin{math}
1.423023 \leq Y \leq 2.796213
\end{math}
\end{center}

# Question 3 - Simple Linear Model

Suppose the linear model we fit is as follows:

\begin{center}
\begin{math}
Y_i = \beta \cdot x_i + e_i
\end{math}
\end{center}

and assume that the least squares estimate $\hat{\beta}$ is given by

\begin{center}
\begin{math}
\hat{\beta} = \frac{\sum\limits_{i=1}^n x_i y_i }{\sum\limits_{i=1}^n x_i^2 }
\end{math}
\end{center}

Then

##### Proof 1: 
\begin{center}
\begin{math}
E(\hat{\beta}|X) = \beta
\end{math}
\end{center}

The proof is as follows:

\begin{center}
\begin{math}
E(\hat{\beta}|X) = 
E(\frac{\sum\limits_{i=1}^n x_i y_i }{\sum\limits_{i=1}^n x_i^2 }|X) = 
\frac{\sum\limits_{i=1}^n x_i E(y_i|X) }{\sum\limits_{i=1}^n x_i^2 } =
\frac{\sum\limits_{i=1}^n x_i \beta x_i }{\sum\limits_{i=1}^n x_i^2 } =
\beta \frac{\sum\limits_{i=1}^n x_i^2 }{\sum\limits_{i=1}^n x_i^2 } =
\beta
\end{math}
\end{center}


##### Proof 2: 
\begin{center}
\begin{math}
Var(\hat{\beta}|X) = \frac{\sigma^2}{\sum\limits_{i=1}^n x_i^2}
\end{math}
\end{center}

The proof is as follows:

\begin{center}
\begin{math}
Var(\hat{\beta}|X) = 
Var(\frac{\sum\limits_{i=1}^n x_i y_i }{\sum\limits_{i=1}^n x_i^2 }|X) = 
Var(\frac{\sum\limits_{i=1}^n x_i (\beta x_i + e_i) }{\sum\limits_{i=1}^n x_i^2 }|X) = 
Var(\frac{\sum\limits_{i=1}^n \beta x_i^2 }{\sum\limits_{i=1}^n x_i^2 } + \frac{\sum\limits_{i=1}^n x_i e_i }{\sum\limits_{i=1}^n x_i^2 }|X) = 
Var(\frac{\sum\limits_{i=1}^n \beta x_i^2 }{\sum\limits_{i=1}^n x_i^2 }|X) + Var(\frac{\sum\limits_{i=1}^n x_i e_i }{\sum\limits_{i=1}^n x_i^2 }|X) = 
0 + \frac{\sum\limits_{i=1}^n x_i^2 Var(e_i|X) }{(\sum\limits_{i=1}^n x_i^2)^2 } = 
\frac{\sum\limits_{i=1}^n x_i^2 \sigma^2 }{(\sum\limits_{i=1}^n x_i^2)^2 } = 
\sigma^2 \frac{\sum\limits_{i=1}^n x_i^2}{(\sum\limits_{i=1}^n x_i^2)^2 } = 
\frac{\sigma^2}{\sum\limits_{i=1}^n x_i^2}
\end{math}
\end{center}


##### Proof 3: 
\begin{center}
\begin{math}
\hat{\beta}|X \sim N(\beta, \frac{\sigma^2}{\sum\limits_{i=1}^n x_i^2})
\end{math}
\end{center}

The proof is as follows:

Combining the above two proofs we can say that:
\begin{center}
\begin{math}
\hat{\beta}|X \sim N(\beta, \frac{\sigma^2}{\sum\limits_{i=1}^n x_i^2})
\end{math}
\end{center}

# Question 4 - Comparision between RSS and SSreg

My Answer: Option d

Explanation: First consider the formulas for RSS and SSreg

\begin{center}
\begin{math}
RSS = \sum\limits_{i=1}^n (Y_i - \hat{Y_i})^2
\end{math}
\end{center}

\begin{center}
\begin{math}
SS_{reg} = \sum\limits_{i=1}^n (\hat{Y_i} - \bar{Y_i})^2
\end{math}
\end{center}

RSS calculates the variability between the actual (observed) values of Y and the predicted values of Y, while SSreg calculates the variability  between the predicted value of Y and mean of Y. In Plot 1 we see that the fitted model is a very close fit to the data, which reduces the value for the difference between actual Y and predicted Y, while in Plot 2 the fitted model is very bad due to which we see large differences between actual Y and fitted Y. This shows that the RSS value for Plot 1 should be less than Plot 2. Now considering SSreg, we see that in Plot 1 the slope of the fitted model is much more steep, hence leading to more difference between predicted Y and mean Y, while the slope of the fitted model in Plot 2 is much more small, due to which there would be less difference between predited Y and mean Y which will reduce the SSreg for Plot 2 as compared to Plot 1 which is why I chose the option D.